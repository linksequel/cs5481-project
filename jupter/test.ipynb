{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T05:28:40.727691Z",
     "start_time": "2025-04-11T05:28:40.698817Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用户-物品矩阵行向量之间的皮尔逊相关系数:\n",
      "userID         1         2         3         4\n",
      "userID                                        \n",
      "1       1.000000  0.593067 -0.148936 -0.565009\n",
      "2       0.593067  1.000000 -0.464140 -0.606168\n",
      "3      -0.148936 -0.464140  1.000000  0.426072\n",
      "4      -0.565009 -0.606168  0.426072  1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建一个简单的用户-物品DataFrame\n",
    "data = {\n",
    "    'userID': [1, 1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4],\n",
    "    'itemID': ['A', 'B', 'C', 'D', 'A', 'C', 'E', 'B', 'C', 'D', 'C', 'D'],\n",
    "    'rating': [5, 3, 0, 1, 4, 0, 2, 1, 0, 5, 5, 4]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 创建用户-物品矩阵\n",
    "user_item_matrix = df.pivot(index='userID', columns='itemID', values='rating').fillna(0)\n",
    "\n",
    "# 计算行向量之间的皮尔逊相关系数\n",
    "pearson_corr = user_item_matrix.T.corr(method='pearson')\n",
    "\n",
    "print(\"用户-物品矩阵行向量之间的皮尔逊相关系数:\")\n",
    "print(pearson_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ce8d5a560d041",
   "metadata": {},
   "source": [
    "# 皮尔逊计算公式\n",
    "皮尔逊相关系数与余弦相似度的计算公式如下：\n",
    "\n",
    "$\n",
    "\\mathrm{pearson}(u, v)=\\frac{\\sum_{i \\in I}(r_{ui}-\\bar{r}_{u})(r_{vi}-\\bar{r}_{v})}{\\sqrt{\\sum_{i \\in I}(r_{ui}-\\bar{r}_{u})^2}\\sqrt{\\sum_{i \\in I}(r_{vi}-\\bar{r}_{v})^2}}\n",
    "$\n",
    "\n",
    "其中：\n",
    "- $(r_{ui}, r_{vi})$ 分别表示用户 \\(u\\) 和用户 \\(v\\) 对物品 \\(i\\) 是否有交互（或具体评分值）；\n",
    "- $(\\bar{r}_{u}, \\bar{r}_{v})$ 分别表示用户 \\(u\\) 和用户 \\(v\\) 交互的所有物品交互数量或者评分的平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f582a41a54899243",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T05:49:25.931738Z",
     "start_time": "2025-04-11T05:49:25.923118Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.9999999999999998\n",
      "-0.9999999999999999\n",
      "单元测试通过\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_pearson_corr(row1, row2):\n",
    "    \"\"\"\n",
    "    计算两个行向量之间的皮尔逊相关系数\n",
    "\n",
    "    参数:\n",
    "    row1 (pd.Series): 第一个行向量\n",
    "    row2 (pd.Series): 第二个行向量\n",
    "\n",
    "    返回:\n",
    "    float: 皮尔逊相关系数\n",
    "    \"\"\"\n",
    "    # 计算均值\n",
    "    mean_row1 = np.mean(row1)\n",
    "    mean_row2 = np.mean(row2)\n",
    "\n",
    "    # 计算分子\n",
    "    numerator = np.sum((row1 - mean_row1) * (row2 - mean_row2))\n",
    "\n",
    "    # 计算分母\n",
    "    denominator = np.sqrt(np.sum((row1 - mean_row1) ** 2)) * np.sqrt(np.sum((row2 - mean_row2) ** 2))\n",
    "\n",
    "    # 计算皮尔逊相关系数\n",
    "    pearson_corr = numerator / denominator\n",
    "\n",
    "    return pearson_corr\n",
    "\n",
    "\n",
    "# 单元测试\n",
    "def test_calculate_pearson_corr():\n",
    "    \"\"\"\n",
    "    测试 calculate_pearson_corr 方法\n",
    "    \"\"\"\n",
    "    # 示例数据\n",
    "    data = {\n",
    "        'userID': [1, 3],\n",
    "        'itemID': ['A', 'B'],\n",
    "        'rating': [5, 2]\n",
    "    }\n",
    "\n",
    "    # 创建用户-物品矩阵\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # 创建用户-物品矩阵\n",
    "    user_item_matrix = df.pivot(index='userID', columns='itemID', values='rating').fillna(0)\n",
    "\n",
    "    # 计算 DataFrame 中两个行向量之间的皮尔逊相关系数\n",
    "    row1 = user_item_matrix.iloc[0]\n",
    "    row2 = user_item_matrix.iloc[1]\n",
    "    pearson_corr = calculate_pearson_corr(row1, row2)\n",
    "    print(pearson_corr)\n",
    "\n",
    "    # 预期结果\n",
    "    expected_corr = np.corrcoef(row1, row2)[0, 1]\n",
    "    print(expected_corr)\n",
    "    assert np.isclose(pearson_corr, expected_corr), f\"Expected {expected_corr}, but got {pearson_corr}\"\n",
    "\n",
    "# 运行单元测试\n",
    "test_calculate_pearson_corr()\n",
    "print(\"单元测试通过\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cfb58acb954a481a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-11T14:12:03.994388Z",
     "start_time": "2025-04-11T14:12:03.932356Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "余弦相似度矩阵:\n",
      "         v1       v3\n",
      "v1  1.00000  0.78026\n",
      "v3  0.78026  1.00000\n",
      "pearson相似度\n",
      "[[ 1.         -0.47673129]\n",
      " [-0.47673129  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# 定义向量\n",
    "v1 = np.array([5, 3, 4, 3, 1])\n",
    "v3 = np.array([3, 1, 3, 3, 5])\n",
    "\n",
    "# 创建DataFrame\n",
    "df = pd.DataFrame([v1, v3], index=['v1', 'v3'])\n",
    "\n",
    "# 计算余弦相似度\n",
    "cosine_sim = cosine_similarity(df)\n",
    "\n",
    "# 将结果转换为DataFrame\n",
    "cosine_sim_df = pd.DataFrame(cosine_sim, index=df.index, columns=df.index)\n",
    "pearson  = np.corrcoef(df)\n",
    "\n",
    "print(\"余弦相似度矩阵:\")\n",
    "print(cosine_sim_df)\n",
    "\n",
    "print(\"pearson相似度\")\n",
    "print(pearson)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167daef9e2342ede",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2025-04-12T15:42:36.051204Z"
    },
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 创建示例DataFrame\n",
    "data = {\n",
    "    'A': [1, 2, 3],\n",
    "    'B': [4, 5, 6],\n",
    "    'C': [7, 8, 9]\n",
    "}\n",
    "df = pd.DataFrame(data, index=['row1', 'row2', 'row3'])\n",
    "# 计算row1的均值\n",
    "row_mean = df.loc['row1'].mean()\n",
    "print(f\"row1的均值: {row_mean}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481ed784",
   "metadata": {},
   "source": [
    "# 测试SVD运用到协同过滤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af7b420a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training SVD model with 20 factors for 20 iterations...\n",
      "Iteration 10/20 completed\n",
      "Iteration 20/20 completed\n",
      "\n",
      "SVD recommendations for user 2:\n",
      "Artist ID: 792, Score: 0.18, Name: Thalía\n",
      "Artist ID: 8388, Score: 0.13, Name: Viking Quest\n",
      "Artist ID: 18121, Score: 0.12, Name: Rytmus\n",
      "Artist ID: 8308, Score: 0.12, Name: Johnny Hallyday\n",
      "Artist ID: 15075, Score: 0.12, Name: 80kidz\n",
      "Artist ID: 14987, Score: 0.12, Name: RICHARD DIXON-COMPOSER\n",
      "Artist ID: 10349, Score: 0.12, Name: Peter Thomas Sound Orchestra\n",
      "Artist ID: 2044, Score: 0.11, Name: Sarah Brightman\n",
      "Artist ID: 14986, Score: 0.11, Name: Dicky Dixon\n",
      "Artist ID: 6696, Score: 0.11, Name: Mara Maravilha\n",
      "Training SVD model with 20 factors for 20 iterations...\n",
      "Iteration 10/20 completed\n",
      "Iteration 20/20 completed\n",
      "\n",
      "Evaluation results:\n",
      "SVD hit rate@10: 0.0000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# Load the dataset\n",
    "def load_dataset(path=\"./datas/hetrec2011-lastfm-2k\"):\n",
    "    user_artists = pd.read_csv(f\"{path}/user_artists.dat\", sep='\\t')\n",
    "    artists = pd.read_csv(f\"{path}/artists.dat\", sep='\\t')\n",
    "    return user_artists, artists\n",
    "\n",
    "# Preprocess data for SVD\n",
    "def preprocess_data_for_svd(user_artists):\n",
    "    # Convert to format needed for SVD\n",
    "    df = user_artists.copy()\n",
    "    # Normalize weights to ratings scale (assuming weight is listen count)\n",
    "    df['rating'] = df['weight'] / df['weight'].max() * 5\n",
    "    df = df.rename(columns={'userID': 'user', 'artistID': 'item'})\n",
    "    return df[['user', 'item', 'rating']]\n",
    "\n",
    "class BiasSVD:\n",
    "    def __init__(self, rating_data, F=10, alpha=0.01, lmbda=0.1, max_iter=50):\n",
    "        self.F = F\n",
    "        self.P = dict()  # User factors\n",
    "        self.Q = dict()  # Item factors\n",
    "        self.bu = dict()  # User biases\n",
    "        self.bi = dict()  # Item biases\n",
    "        self.mu = 0  # Global average\n",
    "        self.alpha = alpha  # Learning rate\n",
    "        self.lmbda = lmbda  # Regularization parameter\n",
    "        self.max_iter = max_iter\n",
    "        self.rating_data = rating_data\n",
    "\n",
    "        # Initialize model parameters\n",
    "        users = rating_data['user'].unique()\n",
    "        items = rating_data['item'].unique()\n",
    "\n",
    "        for user in users:\n",
    "            self.P[user] = [random.random() / math.sqrt(self.F) for _ in range(F)]\n",
    "            self.bu[user] = 0\n",
    "        for item in items:\n",
    "            self.Q[item] = [random.random() / math.sqrt(self.F) for _ in range(F)]\n",
    "            self.bi[item] = 0\n",
    "\n",
    "    def train(self):\n",
    "        # Calculate global mean\n",
    "        self.mu = self.rating_data['rating'].mean()\n",
    "        \n",
    "        print(f\"Training SVD model with {self.F} factors for {self.max_iter} iterations...\")\n",
    "        for step in range(self.max_iter):\n",
    "            # Shuffle data to improve convergence\n",
    "            self.rating_data = self.rating_data.sample(frac=1).reset_index(drop=True)\n",
    "            \n",
    "            for _, row in self.rating_data.iterrows():\n",
    "                user = row['user']\n",
    "                item = row['item']\n",
    "                rating = row['rating']\n",
    "                \n",
    "                # Compute prediction error\n",
    "                pred = self.predict(user, item)\n",
    "                error = rating - pred\n",
    "                \n",
    "                # Update biases\n",
    "                self.bu[user] += self.alpha * (error - self.lmbda * self.bu[user])\n",
    "                self.bi[item] += self.alpha * (error - self.lmbda * self.bi[item])\n",
    "                \n",
    "                # Update latent factors\n",
    "                for f in range(self.F):\n",
    "                    p_old = self.P[user][f]\n",
    "                    q_old = self.Q[item][f]\n",
    "                    \n",
    "                    self.P[user][f] += self.alpha * (error * q_old - self.lmbda * p_old)\n",
    "                    self.Q[item][f] += self.alpha * (error * p_old - self.lmbda * q_old)\n",
    "            \n",
    "            # Decay learning rate\n",
    "            if (step + 1) % 10 == 0:\n",
    "                self.alpha *= 0.9\n",
    "                print(f\"Iteration {step+1}/{self.max_iter} completed\")\n",
    "\n",
    "    def predict(self, user, item):\n",
    "        # Handle users or items not in training set\n",
    "        if user not in self.P or item not in self.Q:\n",
    "            return self.mu\n",
    "            \n",
    "        user_bias = self.bu.get(user, 0)\n",
    "        item_bias = self.bi.get(item, 0)\n",
    "        \n",
    "        # Dot product of user and item latent factors\n",
    "        interaction = sum(self.P[user][f] * self.Q[item][f] for f in range(self.F))\n",
    "        \n",
    "        return self.mu + user_bias + item_bias + interaction\n",
    "\n",
    "    def recommend_items(self, user_id, n_recommendations=10, exclude_items=None):\n",
    "        if exclude_items is None:\n",
    "            exclude_items = set()\n",
    "        else:\n",
    "            exclude_items = set(exclude_items)\n",
    "            \n",
    "        if user_id not in self.P:\n",
    "            return []  # User not in model\n",
    "            \n",
    "        # Get predictions for all items\n",
    "        recommendations = []\n",
    "        for item in self.Q:\n",
    "            if item not in exclude_items:\n",
    "                pred_rating = self.predict(user_id, item)\n",
    "                recommendations.append((item, pred_rating))\n",
    "        \n",
    "        # Sort by predicted rating\n",
    "        recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "        return recommendations[:n_recommendations]\n",
    "\n",
    "# Evaluate model using hit rate\n",
    "def evaluate_hr_k(svd_model, user_item_df, test_ratio=0.2, n_recommendations=10):\n",
    "    # Create a user-item matrix\n",
    "    user_item_matrix = user_item_df.pivot(index='user', columns='item', values='rating').fillna(0)\n",
    "    \n",
    "    test_set = []\n",
    "    train_data = user_item_df.copy()\n",
    "    \n",
    "    # Split data into train and test\n",
    "    for user in user_item_matrix.index:\n",
    "        user_items = user_item_matrix.loc[user]\n",
    "        rated_items = user_items[user_items > 0].index.tolist()\n",
    "        \n",
    "        if len(rated_items) > 5:  # Only test users with enough interactions\n",
    "            # Sample items for testing\n",
    "            test_items = np.random.choice(rated_items, size=int(len(rated_items) * test_ratio), replace=False)\n",
    "            for item in test_items:\n",
    "                test_set.append((user, item, user_item_matrix.loc[user, item]))\n",
    "                # Remove from training set\n",
    "                train_data = train_data[~((train_data['user'] == user) & (train_data['item'] == item))]\n",
    "    \n",
    "    # Sample a smaller test set for efficiency\n",
    "    test_set = random.sample(test_set, min(100, len(test_set)))\n",
    "    \n",
    "    # Train model on reduced dataset\n",
    "    svd_model.rating_data = train_data\n",
    "    svd_model.train()\n",
    "    \n",
    "    # Evaluate\n",
    "    hit_count = 0\n",
    "    for user, item, _ in test_set:\n",
    "        # Get user's rated items in training data\n",
    "        user_rated_items = train_data[train_data['user'] == user]['item'].tolist()\n",
    "        recommendations = svd_model.recommend_items(user, n_recommendations, exclude_items=user_rated_items)\n",
    "        recommended_items = [rec_item for rec_item, _ in recommendations]\n",
    "        \n",
    "        if item in recommended_items:\n",
    "            hit_count += 1\n",
    "            \n",
    "    hit_rate = hit_count / len(test_set) if test_set else 0\n",
    "    return hit_rate\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    user_artists, artists = load_dataset()\n",
    "    \n",
    "    # Prepare data for SVD\n",
    "    rating_data = preprocess_data_for_svd(user_artists)\n",
    "    \n",
    "    # Create and train SVD model\n",
    "    svd_model = BiasSVD(rating_data, F=20, alpha=0.01, lmbda=0.1, max_iter=20)\n",
    "    svd_model.train()\n",
    "    \n",
    "    # Example: Get recommendations for a specific user\n",
    "    user_id = rating_data['user'].iloc[0]  # First user in the dataset\n",
    "    print(f\"\\nSVD recommendations for user {user_id}:\")\n",
    "    \n",
    "    # Get items user has already rated\n",
    "    user_rated_items = rating_data[rating_data['user'] == user_id]['item'].tolist()\n",
    "    recommendations = svd_model.recommend_items(user_id, n_recommendations=10, exclude_items=user_rated_items)\n",
    "    \n",
    "    for item_id, score in recommendations:\n",
    "        artist_name = artists[artists['id'] == item_id]['name'].values[0] if item_id in artists['id'].values else \"Unknown\"\n",
    "        print(f\"Artist ID: {item_id}, Score: {score:.2f}, Name: {artist_name}\")\n",
    "    \n",
    "    # Evaluate the model\n",
    "    hit_rate = evaluate_hr_k(svd_model, rating_data)\n",
    "    print(f\"\\nEvaluation results:\")\n",
    "    print(f\"SVD hit rate@10: {hit_rate:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4740709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    with open('project_structure.txt', 'w') as f:\n",
    "        for root, dirs, files in os.walk(startpath):\n",
    "            level = root.replace(startpath, '').count(os.sep)\n",
    "            indent = ' ' * 4 * (level)\n",
    "            f.write(f'{indent}{os.path.basename(root)}/\\n')\n",
    "            subindent = ' ' * 4 * (level + 1)\n",
    "            for file in files:\n",
    "                f.write(f'{subindent}{file}\\n')\n",
    "\n",
    "# 将这里的path_to_project替换为实际的项目路径\n",
    "list_files('path_to_project')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cityu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
