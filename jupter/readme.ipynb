{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "437491dc",
   "metadata": {},
   "source": [
    "# Author\n",
    "- Name: Su Qiu Lin\n",
    "- Number: 72405483"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3921fb86623730d3",
   "metadata": {},
   "source": [
    "# Content Introduction\n",
    "This part is about Collaborative Filtering(itemcf/usercf) and Matrix Factorization(svd), based on two datasets: lastfm and ml32m.\n",
    "- To run and reproduction\n",
    "    1. Install the requirements.txt\n",
    "  2. Rename datasets and modify the path of the dataset in the code(Optional)\n",
    "       Due to the difference of operating systems, the path may be different.\n",
    "        - def load_lastfm:  **path=\"./datas/hetrec2011-lastfm-2k\"** or **path=\"../datas/hetrec2011-lastfm-2k\"**\n",
    "        - def load_ml32m:  **path=\"./datas/ml-32m\"** or **path=\"../datas/ml-32m\"**\n",
    "    3. Run lastfm.py and ml32m.py file independently.\n",
    "- This ipynb file is just a snapshot of the two .py files' computational results.\n",
    "\n",
    "# Project Structure\n",
    "collaborative/\n",
    "- lastfm.py\n",
    "- ml32m.py\n",
    "- readme.ipynb\n",
    "- requirements.txt - list of dependencies, using \"pip install -r requirements.txt\" to install all dependencies\n",
    "\n",
    "datas/\n",
    "\n",
    "- hetrec2011-lastfm-2k/\n",
    "    - user_artists.dat\n",
    "    - artists.dat\n",
    "- ml-32m/\n",
    "    - ratings.csv\n",
    "    - movies.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4194fd7c4e0ec2f6",
   "metadata": {},
   "source": [
    "# Summary\n",
    "| Method | Advantages | Disadvantages |\n",
    "|--------|------------|---------------|\n",
    "| **ItemCF** | • Good performance for stable item preferences<br>• Strong interpretability of recommendations<br>• No need to recalculate when new users join<br>• Well-suited for items with rich features<br>• Pre-computed similarities can be stored | • Poor performance with sparse item data<br>• Performance degrades with large item catalogs<br>• Cold-start problem for new items<br>• Cannot capture latent factors<br>• Limited diversity in recommendations |\n",
    "| **UserCF** | • Works well with sparse user data<br>• Can discover users' emerging interests<br>• Sensitive to changes in user preferences<br>• Intuitive algorithm concept<br>• Good for social recommendations | • Poor scalability with large user bases<br>• Sensitive to popular items<br>• Cold-start problem for new users<br>• Requires frequent recalculation<br>• Accuracy decreases when user interests are diverse |\n",
    "| **MF (Matrix Factorization)** | • Strong ability to handle sparse data<br>• Can discover latent feature factors<br>• Good scalability for large datasets<br>• High prediction accuracy<br>• Effective with high-dimensional features | • Poor interpretability of recommendations<br>• Cold-start problem for new users/items<br>• Requires hyperparameter tuning<br>• High computational cost for model training<br>• Difficult to update in real-time |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2fad876232fad65",
   "metadata": {},
   "source": [
    "# Experiment Snapshot\n",
    "## 1. lastfm.py - ItemCF/UserCF/SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d7992ce0f172c0a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T11:16:54.639580Z",
     "start_time": "2025-04-14T11:11:27.536219Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User-based recommendations for user 2:\n",
      "Artist ID: 511, Score: 11033.60, Name: U2\n",
      "Artist ID: 159, Score: 9444.82, Name: The Cure\n",
      "Artist ID: 1001, Score: 8645.83, Name: Pet Shop Boys\n",
      "Artist ID: 2562, Score: 6262.49, Name: Arcadia\n",
      "Artist ID: 1014, Score: 5252.81, Name: Erasure\n",
      "Artist ID: 993, Score: 5032.88, Name: Simple Minds\n",
      "Artist ID: 187, Score: 4859.54, Name: a-ha\n",
      "Artist ID: 4313, Score: 4470.64, Name: Nephew\n",
      "Artist ID: 227, Score: 3800.56, Name: The Beatles\n",
      "Artist ID: 6776, Score: 3569.32, Name: Book of Love\n",
      "\n",
      "Item-based recommendations for user 2:\n",
      "Artist ID: 2556, Score: 14102.14, Name: The Power Station\n",
      "Artist ID: 8995, Score: 13904.49, Name: Andy Taylor\n",
      "Artist ID: 1076, Score: 13796.74, Name: Wham!\n",
      "Artist ID: 2562, Score: 13181.35, Name: Arcadia\n",
      "Artist ID: 13161, Score: 12606.11, Name: Private\n",
      "Artist ID: 4313, Score: 12600.67, Name: Nephew\n",
      "Artist ID: 6350, Score: 12455.23, Name: TV-2\n",
      "Artist ID: 996, Score: 11801.53, Name: Mike & The Mechanics\n",
      "Artist ID: 4042, Score: 6047.56, Name: Damn Arms\n",
      "Artist ID: 4023, Score: 6047.56, Name: DJ Risk One\n",
      "\n",
      "SVD-based recommendations for user 2:\n",
      "Artist ID: 3501, Score: 2290.23, Name: Mylène Farmer\n",
      "Artist ID: 1098, Score: 1818.59, Name: Björk\n",
      "Artist ID: 159, Score: 1556.17, Name: The Cure\n",
      "Artist ID: 265, Score: 1553.20, Name: Céline Dion\n",
      "Artist ID: 1001, Score: 1109.75, Name: Pet Shop Boys\n",
      "Artist ID: 475, Score: 1072.39, Name: Eminem\n",
      "Artist ID: 1505, Score: 1050.33, Name: Natalie Imbruglia\n",
      "Artist ID: 7759, Score: 1035.20, Name: Bushido\n",
      "Artist ID: 228, Score: 886.41, Name: Kings of Leon\n",
      "Artist ID: 173, Score: 866.37, Name: Placebo\n",
      "\n",
      "Evaluating recommendation methods...\n",
      "Evaluating recommendations for 50 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 50/50 [05:20<00:00,  6.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results:\n",
      "User-based CF - HR@K: 0.1213, NDCG@K: 0.1421, MRR@K: 0.3611\n",
      "Item-based CF - HR@K: 0.0180, NDCG@K: 0.0237, MRR@K: 0.0833\n",
      "SVD-based CF - HR@K: 0.0949, NDCG@K: 0.1048, MRR@K: 0.2782\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "# Load the dataset\n",
    "def load_lastfm(path=\"../datas/hetrec2011-lastfm-2k\"):\n",
    "    # Load user-artist interactions\n",
    "    user_artists = pd.read_csv(f\"{path}/user_artists.dat\", sep='\\t')\n",
    "\n",
    "    # Load artists data\n",
    "    artists = pd.read_csv(f\"{path}/artists.dat\", sep='\\t')\n",
    "\n",
    "    return user_artists, artists\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(user_artists):\n",
    "    # Create user-item matrix\n",
    "    user_item_matrix_df = user_artists.pivot(index='userID', columns='artistID', values='weight').fillna(0)\n",
    "    return user_item_matrix_df\n",
    "\n",
    "# User-based collaborative filtering\n",
    "# 根据与user_id的相似用户，推荐n个item（artist）给它\n",
    "def user_based_cf(user_item_matrix_df, user_id, n_users=10, n_recommendations=10):\n",
    "    # Calculate user similarity\n",
    "    user_similarity = cosine_similarity(user_item_matrix_df)\n",
    "    user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix_df.index, columns=user_item_matrix_df.index)\n",
    "\n",
    "    # Find similar users\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False)[1:n_users+1].index\n",
    "\n",
    "    # Get recommendations\n",
    "    recommendations = defaultdict(float)\n",
    "\n",
    "    # 对于当前没听过的艺术家，以相似用户喜欢的艺术家频次为准,\n",
    "    # recommendations[item] = 累加（某user和其相似用户的相似度 * weight）\n",
    "    for similar_user in similar_users:\n",
    "        similarity_between_user = user_similarity_df.loc[user_id, similar_user]\n",
    "\n",
    "        for item in user_item_matrix_df.columns:\n",
    "            if user_item_matrix_df.loc[user_id, item] == 0 and user_item_matrix_df.loc[similar_user, item] > 0:\n",
    "                recommendations[item] += similarity_between_user * user_item_matrix_df.loc[similar_user, item]\n",
    "\n",
    "    # Sort recommendations\n",
    "    recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)[:n_recommendations]\n",
    "    return recommendations\n",
    "\n",
    "# Item-based collaborative filtering\n",
    "def item_based_cf(user_item_matrix_df, user_id, n_items=10, n_recommendations=10):\n",
    "    # 1. Calculate item similarity between items\n",
    "    item_similarity = cosine_similarity(user_item_matrix_df.T)\n",
    "    item_similarity_df = pd.DataFrame(item_similarity, index=user_item_matrix_df.columns, columns=user_item_matrix_df.columns)\n",
    "\n",
    "    # 2. Get items the user has interacted with and weights > 0\n",
    "    user_related_items = user_item_matrix_df.loc[user_id]\n",
    "    related_items_id = user_related_items[user_related_items > 0].index.tolist()\n",
    "\n",
    "    # Get recommendations\n",
    "    recommendations = defaultdict(float)\n",
    "\n",
    "    for item_id in related_items_id:\n",
    "        item_weight = user_item_matrix_df.loc[user_id, item_id]\n",
    "\n",
    "        # 2.1 Find n top items similar to current item, according to item_similarity_df\n",
    "        similar_items = item_similarity_df[item_id].sort_values(ascending=False)[1:n_items+1]\n",
    "\n",
    "        for similar_item, similarity_between_item in similar_items.items():\n",
    "            # 2.2 only find the similar item is not in related_items_id, add it to recommendations\n",
    "            if similar_item in related_items_id:\n",
    "                continue\n",
    "            # the below similar_item's weight must be 0\n",
    "            recommendations[similar_item] += similarity_between_item * item_weight\n",
    "\n",
    "    # Sort recommendations\n",
    "    recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)[:n_recommendations]\n",
    "    return recommendations\n",
    "\n",
    "# SVD-based collaborative filtering\n",
    "def svd_based_cf(user_item_matrix_df, user_id, n_factors=50, n_recommendations=10):\n",
    "    # Convert to numpy array for SVD\n",
    "    user_item_array = user_item_matrix_df.values\n",
    "\n",
    "    # Get user index\n",
    "    user_idx = user_item_matrix_df.index.get_loc(user_id)\n",
    "\n",
    "    # Ensure n_factors is not too large\n",
    "    n_factors = min(n_factors, min(user_item_array.shape) - 1)\n",
    "\n",
    "    # Perform SVD\n",
    "    u, sigma, vt = svds(user_item_array, k=n_factors)\n",
    "\n",
    "    # Convert to diagonal matrix\n",
    "    sigma_diag = np.diag(sigma)\n",
    "\n",
    "    # Reconstruct the matrix using the factors\n",
    "    reconstructed_matrix = np.dot(np.dot(u, sigma_diag), vt)\n",
    "\n",
    "    # Get the reconstructed ratings for the user\n",
    "    user_ratings = reconstructed_matrix[user_idx]\n",
    "\n",
    "    # Get actual ratings for the user\n",
    "    actual_ratings = user_item_array[user_idx]\n",
    "\n",
    "    # Create a mask of items the user hasn't interacted with\n",
    "    unrated_items = actual_ratings == 0\n",
    "\n",
    "    # Get indices of items the user hasn't interacted with, ordered by predicted rating\n",
    "    candidate_items = np.argsort(-user_ratings * unrated_items)[:n_recommendations]\n",
    "\n",
    "    # Convert back to original item IDs and predicted scores\n",
    "    recommendations = [(user_item_matrix_df.columns[item_idx], user_ratings[item_idx])\n",
    "                      for item_idx in candidate_items if unrated_items[item_idx]]\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Evaluate recommendations\n",
    "def evaluate(user_item_matrix, test_ratio=0.2, n_users=10, n_items=10, n_factors=50, n_recommendations=10):\n",
    "    # Create a copy of the matrix to avoid modifying the original\n",
    "    matrix = user_item_matrix.copy()\n",
    "\n",
    "    # Metrics storage\n",
    "    metrics = {\n",
    "        'user_hr': [], 'user_ndcg': [], 'user_mrr': [],\n",
    "        'item_hr': [], 'item_ndcg': [], 'item_mrr': [],\n",
    "        'svd_hr': [], 'svd_ndcg': [], 'svd_mrr': []\n",
    "    }\n",
    "    # if not sample, cause 3 hours to execute evaluate but get similar results with using sample...\n",
    "    sampled_users = random.sample(list(matrix.index), 50)\n",
    "    print(f\"Evaluating recommendations for {len(sampled_users)} users...\")\n",
    "\n",
    "    # For each user, hide some interactions as test data\n",
    "    for user_id in tqdm(sampled_users, desc=\"Evaluating\", ncols=80):\n",
    "        # Get items this user has interacted with\n",
    "        user_items = matrix.columns[matrix.loc[user_id] > 0].tolist()\n",
    "\n",
    "        # Skip users with too few interactions\n",
    "        if len(user_items) <= 2:\n",
    "            continue\n",
    "\n",
    "        # Randomly select items for testing\n",
    "        n_test = max(1, int(len(user_items) * test_ratio))\n",
    "        test_items = random.sample(user_items, n_test)\n",
    "\n",
    "        # Create training matrix by setting test items to zero\n",
    "        train_matrix = matrix.copy()\n",
    "        for item in test_items:\n",
    "            train_matrix.loc[user_id, item] = 0\n",
    "\n",
    "        # Get recommendations from all methods\n",
    "        user_recs = user_based_cf(train_matrix, user_id, n_users, n_recommendations)\n",
    "        item_recs = item_based_cf(train_matrix, user_id, n_items, n_recommendations)\n",
    "        svd_recs = svd_based_cf(train_matrix, user_id, n_factors, n_recommendations)\n",
    "\n",
    "        # Extract just the item IDs\n",
    "        user_rec_items = [item_id for item_id, _ in user_recs]\n",
    "        item_rec_items = [item_id for item_id, _ in item_recs]\n",
    "        svd_rec_items = [item_id for item_id, _ in svd_recs]\n",
    "\n",
    "        # Calculate metrics for user-based CF\n",
    "        metrics['user_hr'].append(hit_ratio(user_rec_items, test_items))\n",
    "        metrics['user_ndcg'].append(ndcg(user_rec_items, test_items))\n",
    "        metrics['user_mrr'].append(mrr(user_rec_items, test_items))\n",
    "\n",
    "        # Calculate metrics for item-based CF\n",
    "        metrics['item_hr'].append(hit_ratio(item_rec_items, test_items))\n",
    "        metrics['item_ndcg'].append(ndcg(item_rec_items, test_items))\n",
    "        metrics['item_mrr'].append(mrr(item_rec_items, test_items))\n",
    "\n",
    "        # Calculate metrics for SVD-based CF\n",
    "        metrics['svd_hr'].append(hit_ratio(svd_rec_items, test_items))\n",
    "        metrics['svd_ndcg'].append(ndcg(svd_rec_items, test_items))\n",
    "        metrics['svd_mrr'].append(mrr(svd_rec_items, test_items))\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {k: np.mean(v) for k, v in metrics.items() if v}\n",
    "\n",
    "    return avg_metrics\n",
    "\n",
    "# Hit Ratio@K\n",
    "def hit_ratio(recommended_items, test_items):\n",
    "    hits = len(set(recommended_items) & set(test_items))\n",
    "    return hits / len(test_items) if test_items else 0\n",
    "\n",
    "# NDCG@K\n",
    "def ndcg(recommended_items, test_items):\n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "\n",
    "    # Calculate DCG\n",
    "    for i, item in enumerate(recommended_items):\n",
    "        if item in test_items:\n",
    "            # Using binary relevance (1 if hit, 0 if miss)\n",
    "            dcg += 1 / np.log2(i + 2)  # i+2 because i starts from 0\n",
    "\n",
    "    # Calculate IDCG (ideal DCG - items are perfectly ranked)\n",
    "    for i in range(min(len(test_items), len(recommended_items))):\n",
    "        idcg += 1 / np.log2(i + 2)\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "# MRR@K\n",
    "def mrr(recommended_items, test_items):\n",
    "    for i, item in enumerate(recommended_items):\n",
    "        if item in test_items:\n",
    "            return 1 / (i + 1)  # i+1 because i starts from 0\n",
    "    return 0\n",
    "\n",
    "def load_and_preprocess_data():\n",
    "    user_artists, artists = load_lastfm()\n",
    "    user_item_matrix_df = preprocess_data(user_artists)\n",
    "    return user_artists, artists, user_item_matrix_df\n",
    "\n",
    "def print_recommendations(user_id, recommendations, artists, method_name):\n",
    "    print(f\"\\n{method_name} recommendations for user {user_id}:\")\n",
    "    for item_id, score in recommendations:\n",
    "        artist_name = artists[artists['id'] == item_id]['name'].values[0] if item_id in artists['id'].values else \"Unknown\"\n",
    "        print(f\"Artist ID: {item_id}, Score: {score:.2f}, Name: {artist_name}\")\n",
    "\n",
    "def generate_recommendations(user_item_matrix_df, user_id, artists):\n",
    "    # Get recommendations using different methods\n",
    "    user_recommendations = user_based_cf(user_item_matrix_df, user_id)\n",
    "    print_recommendations(user_id, user_recommendations, artists, \"User-based\")\n",
    "\n",
    "    item_recommendations = item_based_cf(user_item_matrix_df, user_id)\n",
    "    print_recommendations(user_id, item_recommendations, artists, \"Item-based\")\n",
    "\n",
    "    svd_recommendations = svd_based_cf(user_item_matrix_df, user_id)\n",
    "    print_recommendations(user_id, svd_recommendations, artists, \"SVD-based\")\n",
    "\n",
    "def evaluate_models(user_item_matrix_df):\n",
    "    print(\"\\nEvaluating recommendation methods...\")\n",
    "    metrics = evaluate(user_item_matrix_df)\n",
    "    print(f\"\\nEvaluation results:\")\n",
    "    print(f\"User-based CF - HR@K: {metrics['user_hr']:.4f}, NDCG@K: {metrics['user_ndcg']:.4f}, MRR@K: {metrics['user_mrr']:.4f}\")\n",
    "    print(f\"Item-based CF - HR@K: {metrics['item_hr']:.4f}, NDCG@K: {metrics['item_ndcg']:.4f}, MRR@K: {metrics['item_mrr']:.4f}\")\n",
    "    print(f\"SVD-based CF - HR@K: {metrics['svd_hr']:.4f}, NDCG@K: {metrics['svd_ndcg']:.4f}, MRR@K: {metrics['svd_mrr']:.4f}\")\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    user_artists, artists, user_item_matrix_df = load_and_preprocess_data()\n",
    "\n",
    "    # Example: Get recommendations for a specific user\n",
    "    user_id = user_item_matrix_df.index[0]  # First user in the dataset\n",
    "\n",
    "    # Generate and print recommendations\n",
    "    generate_recommendations(user_item_matrix_df, user_id, artists)\n",
    "\n",
    "    # Evaluate the models\n",
    "    evaluate_models(user_item_matrix_df)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f252b696de87f64",
   "metadata": {},
   "source": [
    "# 2. ml32m.py - UserCF/SVD\n",
    "not suitable for itemCF because the movie items are too many"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c934534e56681c36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-14T11:48:50.307958Z",
     "start_time": "2025-04-14T11:39:27.017604Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ML-32M dataset...\n",
      "Using a sample of 10000 users for analysis...\n",
      "Preprocessing data...\n",
      "\n",
      "Generating recommendations for user 10...\n",
      "\n",
      "User-based recommendations for user 10:\n",
      "Movie ID: 3147, Score: 19.81, Title: Green Mile, The (1999)\n",
      "Movie ID: 54286, Score: 19.78, Title: Bourne Ultimatum, The (2007)\n",
      "Movie ID: 8961, Score: 19.76, Title: Incredibles, The (2004)\n",
      "Movie ID: 1676, Score: 19.75, Title: Starship Troopers (1997)\n",
      "Movie ID: 2858, Score: 19.70, Title: American Beauty (1999)\n",
      "Movie ID: 4011, Score: 19.51, Title: Snatch (2000)\n",
      "Movie ID: 59784, Score: 19.50, Title: Kung Fu Panda (2008)\n",
      "Movie ID: 69844, Score: 19.23, Title: Harry Potter and the Half-Blood Prince (2009)\n",
      "Movie ID: 4886, Score: 19.18, Title: Monsters, Inc. (2001)\n",
      "Movie ID: 64614, Score: 18.96, Title: Gran Torino (2008)\n",
      "\n",
      "SVD-based recommendations for user 10:\n",
      "Movie ID: 69844, Score: 3.15, Title: Harry Potter and the Half-Blood Prince (2009)\n",
      "Movie ID: 81834, Score: 2.92, Title: Harry Potter and the Deathly Hallows: Part 1 (2010)\n",
      "Movie ID: 88125, Score: 2.79, Title: Harry Potter and the Deathly Hallows: Part 2 (2011)\n",
      "Movie ID: 54286, Score: 2.67, Title: Bourne Ultimatum, The (2007)\n",
      "Movie ID: 2858, Score: 2.54, Title: American Beauty (1999)\n",
      "Movie ID: 3147, Score: 2.45, Title: Green Mile, The (1999)\n",
      "Movie ID: 1136, Score: 2.33, Title: Monty Python and the Holy Grail (1975)\n",
      "Movie ID: 3535, Score: 2.24, Title: American Psycho (2000)\n",
      "Movie ID: 60074, Score: 2.23, Title: Hancock (2008)\n",
      "Movie ID: 30793, Score: 2.20, Title: Charlie and the Chocolate Factory (2005)\n",
      "\n",
      "Evaluating recommendation methods...\n",
      "Evaluating recommendations for 50 users...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|███████████████████████████████| 50/50 [08:56<00:00, 10.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation results:\n",
      "User-based CF - HR@K: 0.0434, NDCG@K: 0.7641, MRR@K: 0.9667\n",
      "SVD-based CF - HR@K: 0.0525, NDCG@K: 0.8940, MRR@K: 0.9567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from scipy.sparse import csr_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "\n",
    "\"\"\"\n",
    "author: suqiulin-72405483\n",
    "- Perform collaborative recommendation algorithm(usercf/svd) on this dataset\n",
    "    - itemCF won't be implement because there are 292757 movies(item) which is very slow performed on traditional cpu\n",
    "\"\"\"\n",
    "# Load the ML-32M dataset\n",
    "def load_ml32m(path=\"../datas/ml-32m\"):\n",
    "    print(\"Loading ML-32M dataset...\")\n",
    "    # Load ratings data in chunks due to large size\n",
    "    ratings = pd.read_csv(f\"{path}/ratings.csv\", chunksize=1000000)\n",
    "    ratings_df = pd.concat(ratings)\n",
    "    # Optional: load movie data if available\n",
    "    try:\n",
    "        movies = pd.read_csv(f\"{path}/movies.csv\")\n",
    "    except:\n",
    "        movies = None\n",
    "        print(\"Movies data not found. Will continue without movie titles.\")\n",
    "\n",
    "    return ratings_df, movies\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(ratings_df, sample_size=None):\n",
    "    print(\"Preprocessing data...\")\n",
    "    if sample_size:\n",
    "        # Take a sample if full dataset is too large\n",
    "        user_counts = ratings_df['userId'].value_counts()\n",
    "        users_to_keep = user_counts[user_counts >= 5].index[:sample_size]\n",
    "        ratings_df = ratings_df[ratings_df['userId'].isin(users_to_keep)]\n",
    "\n",
    "    # Create sparse user-item matrix\n",
    "    users = ratings_df['userId'].unique()\n",
    "    items = ratings_df['movieId'].unique()\n",
    "\n",
    "    user_mapper = {user: i for i, user in enumerate(users)}\n",
    "    item_mapper = {item: i for i, item in enumerate(items)}\n",
    "\n",
    "    user_indices = [user_mapper[user] for user in ratings_df['userId']]\n",
    "    item_indices = [item_mapper[item] for item in ratings_df['movieId']]\n",
    "\n",
    "    # Create sparse matrix\n",
    "    matrix = csr_matrix((ratings_df['rating'], (user_indices, item_indices)),\n",
    "                        shape=(len(users), len(items)))\n",
    "\n",
    "    return matrix, users, items, user_mapper, item_mapper\n",
    "\n",
    "# User-based collaborative filtering\n",
    "def user_based_cf(matrix, user_idx, n_users=10, n_recommendations=10):\n",
    "    # Get user row\n",
    "    user_row = matrix[user_idx].toarray().flatten()\n",
    "\n",
    "    # Calculate similarities\n",
    "    similarities = []\n",
    "    for i in range(matrix.shape[0]):\n",
    "        if i != user_idx:\n",
    "            other_row = matrix[i].toarray().flatten()\n",
    "            # Compute similarity only if users have common items\n",
    "            if np.sum(user_row > 0) > 0 and np.sum(other_row > 0) > 0:\n",
    "                sim = cosine_similarity(user_row.reshape(1, -1), other_row.reshape(1, -1))[0][0]\n",
    "                similarities.append((i, sim))\n",
    "\n",
    "    # Sort by similarity\n",
    "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "    similar_users = similarities[:n_users]\n",
    "\n",
    "    # Get recommendations\n",
    "    recommendations = defaultdict(float)\n",
    "    for similar_user, similarity in similar_users:\n",
    "        similar_user_row = matrix[similar_user].toarray().flatten()\n",
    "        for item_idx in range(len(similar_user_row)):\n",
    "            if user_row[item_idx] == 0 and similar_user_row[item_idx] > 0:\n",
    "                recommendations[item_idx] += similarity * similar_user_row[item_idx]\n",
    "\n",
    "    # Sort recommendations\n",
    "    recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)[:n_recommendations]\n",
    "    return recommendations\n",
    "\n",
    "# Item-based collaborative filtering\n",
    "def item_based_cf(matrix, user_idx, n_items=10, n_recommendations=10):\n",
    "    # Get user's rated items\n",
    "    user_row = matrix[user_idx].toarray().flatten()\n",
    "    user_rated_items = np.where(user_row > 0)[0]\n",
    "\n",
    "    # Get recommendations\n",
    "    recommendations = defaultdict(float)\n",
    "    for item_idx in user_rated_items:\n",
    "        item_col = matrix.T[item_idx].toarray().flatten()\n",
    "\n",
    "        # Find similar items\n",
    "        for other_item_idx in range(matrix.shape[1]):\n",
    "            if other_item_idx != item_idx and user_row[other_item_idx] == 0:\n",
    "                other_item_col = matrix.T[other_item_idx].toarray().flatten()\n",
    "                # Compute similarity only if items have common users\n",
    "                if np.sum(item_col > 0) > 0 and np.sum(other_item_col > 0) > 0:\n",
    "                    sim = cosine_similarity(item_col.reshape(1, -1), other_item_col.reshape(1, -1))[0][0]\n",
    "                    recommendations[other_item_idx] += sim * user_row[item_idx]\n",
    "\n",
    "    # Sort recommendations\n",
    "    recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)[:n_recommendations]\n",
    "    return recommendations\n",
    "\n",
    "# SVD-based collaborative filtering\n",
    "def svd_based_cf(matrix, user_idx, n_factors=50, n_recommendations=10):\n",
    "    # Perform SVD\n",
    "    U, sigma, Vt = svds(matrix, k=n_factors)\n",
    "\n",
    "    # Reconstruct the matrix\n",
    "    sigma_diag = np.diag(sigma)\n",
    "    predicted_ratings = np.dot(np.dot(U, sigma_diag), Vt)\n",
    "\n",
    "    # Get the predicted ratings for user\n",
    "    user_pred_ratings = predicted_ratings[user_idx]\n",
    "\n",
    "    # Get actual ratings\n",
    "    user_row = matrix[user_idx].toarray().flatten()\n",
    "\n",
    "    # Create mask for unrated items\n",
    "    unrated_items = np.where(user_row == 0)[0]\n",
    "\n",
    "    # Get recommendations\n",
    "    recommendations = [(item_idx, user_pred_ratings[item_idx])\n",
    "                      for item_idx in unrated_items]\n",
    "\n",
    "    # Sort by predicted rating\n",
    "    recommendations.sort(key=lambda x: x[1], reverse=True)\n",
    "    return recommendations[:n_recommendations]\n",
    "\n",
    "# Evaluation metrics\n",
    "def hit_ratio(recommended_items, test_items):\n",
    "    hits = len(set(recommended_items) & set(test_items))\n",
    "    return hits / len(test_items) if test_items else 0\n",
    "\n",
    "def ndcg(recommended_items, test_items):\n",
    "    dcg = 0\n",
    "    idcg = 0\n",
    "\n",
    "    for i, item in enumerate(recommended_items):\n",
    "        if item in test_items:\n",
    "            dcg += 1 / np.log2(i + 2)\n",
    "\n",
    "    for i in range(min(len(test_items), len(recommended_items))):\n",
    "        idcg += 1 / np.log2(i + 2)\n",
    "\n",
    "    return dcg / idcg if idcg > 0 else 0\n",
    "\n",
    "def mrr(recommended_items, test_items):\n",
    "    for i, item in enumerate(recommended_items):\n",
    "        if item in test_items:\n",
    "            return 1 / (i + 1)\n",
    "    return 0\n",
    "\n",
    "# Evaluate recommendations\n",
    "def evaluate(matrix, users, test_ratio=0.2, n_users=10, n_items=10, n_factors=50, n_recommendations=10):\n",
    "    metrics = {\n",
    "        'user_hr': [], 'user_ndcg': [], 'user_mrr': [],\n",
    "        'item_hr': [], 'item_ndcg': [], 'item_mrr': [],\n",
    "        'svd_hr': [], 'svd_ndcg': [], 'svd_mrr': []\n",
    "    }\n",
    "\n",
    "    # Sample users for evaluation\n",
    "    sampled_users = random.sample(range(len(users)), min(50, len(users)))\n",
    "    print(f\"Evaluating recommendations for {len(sampled_users)} users...\")\n",
    "\n",
    "    for user_idx in tqdm(sampled_users, desc=\"Evaluating\", ncols=80):\n",
    "        # Get items user has rated\n",
    "        user_row = matrix[user_idx].toarray().flatten()\n",
    "        user_items = np.where(user_row > 0)[0]\n",
    "\n",
    "        # Skip users with too few interactions\n",
    "        if len(user_items) <= 2:\n",
    "            continue\n",
    "\n",
    "        # Select test items\n",
    "        n_test = max(1, int(len(user_items) * test_ratio))\n",
    "        test_items = random.sample(list(user_items), n_test)\n",
    "\n",
    "        # Create training matrix\n",
    "        train_matrix = matrix.copy()\n",
    "        for item_idx in test_items:\n",
    "            train_matrix[user_idx, item_idx] = 0\n",
    "\n",
    "        # Get recommendations\n",
    "        user_recs = user_based_cf(train_matrix, user_idx, n_users, n_recommendations)\n",
    "        # item_recs = item_based_cf(train_matrix, user_idx, n_items, n_recommendations)\n",
    "        svd_recs = svd_based_cf(train_matrix, user_idx, n_factors, n_recommendations)\n",
    "\n",
    "        # Extract just the item indices\n",
    "        user_rec_items = [item_idx for item_idx, _ in user_recs]\n",
    "        svd_rec_items = [item_idx for item_idx, _ in svd_recs]\n",
    "\n",
    "        # Calculate metrics\n",
    "        metrics['user_hr'].append(hit_ratio(user_rec_items, test_items))\n",
    "        metrics['user_ndcg'].append(ndcg(user_rec_items, test_items))\n",
    "        metrics['user_mrr'].append(mrr(user_rec_items, test_items))\n",
    "\n",
    "        metrics['svd_hr'].append(hit_ratio(svd_rec_items, test_items))\n",
    "        metrics['svd_ndcg'].append(ndcg(svd_rec_items, test_items))\n",
    "        metrics['svd_mrr'].append(mrr(svd_rec_items, test_items))\n",
    "\n",
    "    # Calculate average metrics\n",
    "    avg_metrics = {k: np.mean(v) for k, v in metrics.items() if v}\n",
    "    return avg_metrics\n",
    "\n",
    "def print_recommendations(user_id, recommendations, original_items, movies=None, method_name=\"\"):\n",
    "    print(f\"\\n{method_name} recommendations for user {user_id}:\")\n",
    "    for item_idx, score in recommendations:\n",
    "        item_id = original_items[item_idx]\n",
    "        if movies is not None and item_id in movies['movieId'].values:\n",
    "            movie_name = movies[movies['movieId'] == item_id]['title'].values[0]\n",
    "            print(f\"Movie ID: {item_id}, Score: {score:.2f}, Title: {movie_name}\")\n",
    "        else:\n",
    "            print(f\"Movie ID: {item_id}, Score: {score:.2f}\")\n",
    "\n",
    "def main():\n",
    "    # Load and preprocess data\n",
    "    ratings_df, movies = load_ml32m()\n",
    "\n",
    "    # Use a sample of the data if it's too large\n",
    "    #（may generate 16966441536 which will crush the program）\n",
    "    # Adjust based on your hardware capabilities\n",
    "    sample_size = 10000\n",
    "    print(f\"Using a sample of {sample_size} users for analysis...\")\n",
    "\n",
    "    matrix, users, items, user_mapper, item_mapper = preprocess_data(ratings_df, sample_size)\n",
    "\n",
    "    # Example: Get recommendations for a specific user\n",
    "    user_idx = 0  # First user in the sample\n",
    "    original_user_id = users[user_idx]\n",
    "\n",
    "    # Generate recommendations\n",
    "    print(f\"\\nGenerating recommendations for user {original_user_id}...\")\n",
    "\n",
    "    user_recs = user_based_cf(matrix, user_idx)\n",
    "    print_recommendations(original_user_id, user_recs, items, movies, \"User-based\")\n",
    "\n",
    "    svd_recs = svd_based_cf(matrix, user_idx)\n",
    "    print_recommendations(original_user_id, svd_recs, items, movies, \"SVD-based\")\n",
    "\n",
    "    # Evaluate models\n",
    "    print(\"\\nEvaluating recommendation methods...\")\n",
    "    metrics = evaluate(matrix, users)\n",
    "\n",
    "    print(f\"\\nEvaluation results:\")\n",
    "    print(f\"User-based CF - HR@K: {metrics['user_hr']:.4f}, NDCG@K: {metrics['user_ndcg']:.4f}, MRR@K: {metrics['user_mrr']:.4f}\")\n",
    "    print(f\"SVD-based CF - HR@K: {metrics['svd_hr']:.4f}, NDCG@K: {metrics['svd_ndcg']:.4f}, MRR@K: {metrics['svd_mrr']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
